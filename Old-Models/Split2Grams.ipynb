{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return nlp(x, parse=False, tag=False, entity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/wikitext-2/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    with open(data_file) as d:\n",
    "        for line in d:\n",
    "            yield tokenize(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = load_file(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(file):\n",
    "    tokens = {}\n",
    "    for sent in file:\n",
    "        for token in sent:\n",
    "            try:\n",
    "                tokens[token.text] += 1\n",
    "            except:\n",
    "                tokens[token.text] = 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tokens = get_tokens(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def norm_probs(tokens):\n",
    "    s = sum(tokens.values())\n",
    "    return {k : round(math.log(v/float(s)),4) for k, v in tokens.items()}\n",
    "def sort_probs(probs):\n",
    "    return sorted(probs.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = norm_probs(tokens)\n",
    "sorted_probs = sort_probs(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, min_length = 5, max_N=4):\n",
    "    for token, freq in tokens:\n",
    "        if len(token) >= min_length:\n",
    "            splits = []\n",
    "            for i in range(2, max_N+1):\n",
    "                splits += [list([''.join(x) for x in nltk.ngrams(list(token), i)])]\n",
    "            yield token, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_encoded(token_len,splits):\n",
    "    new_splits = {}\n",
    "    for ng_split in splits:\n",
    "        for pos, split in enumerate(ng_split):\n",
    "            if split not in new_splits:\n",
    "                new_splits[split] = 0\n",
    "            new_splits[split] += pos/float(token_len)\n",
    "    return new_splits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_splits = {}\n",
    "for token, splits in get_ngrams(sorted_probs):\n",
    "    all_splits[token] = get_pos_encoded(len(token), splits)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = []\n",
    "for key, value in all_splits.items():\n",
    "    row = pd.Series(value).reset_index()\n",
    "    row.insert(0, 'word', key)\n",
    "    result.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counts = result['index'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = split_counts.index.str.len().unique()\n",
    "splits_by_len = [split_counts[split_counts.index.str.len() == x] for x in lens]\n",
    "splits_by_len = [x / (x.sum() * (i+2)) for i,x in enumerate(splits_by_len)]\n",
    "split_counts = pd.concat(splits_by_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_counts = split_counts / split_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_quants = split_counts.quantile([0.25, 0.4, 0.6, 0.75, 0.85, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_splits = split_counts[split_counts > split_quants[0.6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_splits.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = result['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_splits.index.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[result['index'].isin(pruned_splits.index)].str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_result = result[result['index'].isin(pruned_splits.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = len(pruned_result['word'].unique())\n",
    "s_count = pruned_splits.count()\n",
    "word_count, s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "piv = pd.pivot_table(pruned_result, index=['word'], columns=['index']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_split_m = piv.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500, n_iter=7, random_state=42)\n",
    "word_split_red = svd.fit_transform(word_split_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_split_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_m = numpy.ndarray((word_split_m.shape[1], word_split_red.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piv.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = word_split_m[0]\n",
    "for i in range(split_m.shape[0]):\n",
    "    split_v = word_split_red[word_split_m[:,i] > 0].sum(axis=0)\n",
    "    if numpy.linalg.norm(split_v) > 0:\n",
    "        split_v = split_v / numpy.linalg.norm(split_v)\n",
    "    split_m[i] = split_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2index = lambda x : (piv.columns.get_level_values(1) == x).argmax()\n",
    "index2split = lambda x : piv.columns[x]\n",
    "word2vector = lambda e : word_split_red[(piv.index == e).argmax()]\n",
    "split2vector = lambda e : split_m[split2index(e)]\n",
    "index2word = lambda x : piv.index[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 6652\n",
    "w = split_m[ind]\n",
    "splt = index2split(ind)\n",
    "print(splt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.dot(split_m.T).argsort()[-5:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2split(13410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = piv.index[3001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "e = 'eating'\n",
    "evec = word2vector(e)\n",
    "possible_splits = pruned_result[pruned_result['word'] == e]['index']\n",
    "distances = sorted(((p, scipy.spatial.distance.cosine(evec, split2vector(p))) for p in possible_splits),key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2index('Edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
